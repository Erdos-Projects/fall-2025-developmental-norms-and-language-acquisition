{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd30f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: editdistance in c:\\users\\benar\\anaconda3\\envs\\erdos_ds_environment\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: unidecode in c:\\users\\benar\\anaconda3\\envs\\erdos_ds_environment\\lib\\site-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run pip install to get libraries:\n",
    "%pip install editdistance unidecode\n",
    "\n",
    "# Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "import editdistance\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac81bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "PARQUET_TRAIN = \"data/processed/train_slam_with_features.parquet\"\n",
    "PARQUET_DEV   = \"data/processed/dev_slam_with_features.parquet\"\n",
    "PARQUET_TEST  = \"data/processed/test_slam_with_features.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf46fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "train = pd.read_parquet(PARQUET_TRAIN)\n",
    "dev   = pd.read_parquet(PARQUET_DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42969504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def add_editdistance_features(df, trans):\\n    merged = df.merge(trans, on=\"uni_lemma\", how=\"left\")\\n    for lang in [\"French\", \"Spanish\"]:\\n        d_col = f\"edit_{lang.lower()}\"\\n        f_col = f\"edit_{lang.lower()}_frac\"\\n        merged[d_col] = merged.apply(\\n            lambda r: editdistance.eval(str(r[\"English\"]), str(r[lang]))\\n            if pd.notna(r[\"English\"]) and pd.notna(r[lang]) else np.nan,\\n            axis=1\\n        )\\n        merged[f_col] = merged.apply(\\n            lambda r: r[d_col] / max(len(str(r[\"English\"])), len(str(r[lang])), 1)\\n            if pd.notna(r[d_col]) else np.nan,\\n            axis=1\\n        )\\n    return merged\\n\\ntrain = add_editdistance_features(train, trans)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Word Comparisons File\n",
    "trans = pd.read_csv(\"data/processed/language_translation_table.csv\")\n",
    "\n",
    "# maps from translation table\n",
    "en_map = dict(zip(trans[\"uni_lemma\"], trans[\"English\"].astype(str)))\n",
    "fr_map = dict(zip(trans[\"uni_lemma\"], trans[\"French\"].astype(str)))\n",
    "es_map = dict(zip(trans[\"uni_lemma\"], trans[\"Spanish\"].astype(str)))\n",
    "\n",
    "def norm(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = unidecode(str(s)).lower().strip()\n",
    "    parts = s.split()\n",
    "    if len(parts) == 2 and parts[0] in {\"to\",\"i\",\"we\",\"you\",\"they\",\"he\",\"she\",\"the\",\"i'll\"}:\n",
    "        s = parts[1]\n",
    "    elif len(parts) > 2 and len(parts) > 1 and parts[1] == \"will\":\n",
    "        s = \" \".join(parts[2:])\n",
    "    return s\n",
    " \n",
    "def add_edit_distance_features(df):\n",
    "    key = df[\"uni_lemma\"]\n",
    "\n",
    "    # english and target by l2\n",
    "    eng = key.map(en_map)\n",
    "    tgt = np.where(df[\"l2\"].eq(\"fr\"), key.map(fr_map),\n",
    "        np.where(df[\"l2\"].eq(\"es\"), key.map(es_map), np.nan))\n",
    "\n",
    "    eng_n = pd.Series(eng).map(norm)\n",
    "    tgt_n = pd.Series(tgt).map(norm)\n",
    "\n",
    "    mask = eng_n.notna() & tgt_n.notna()\n",
    "    dist = np.full(len(df), np.nan, dtype=float)\n",
    "    dist[mask.values] = [editdistance.eval(a, b) for a, b in zip(eng_n[mask], tgt_n[mask])]\n",
    "    maxlen = np.maximum(eng_n.str.len(), tgt_n.str.len()).replace(0, 1)\n",
    "    frac = dist / maxlen.to_numpy()\n",
    "\n",
    "    df[\"edit_l2\"] = dist\n",
    "    df[\"edit_l2_frac\"] = frac\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_edit_distance_features(train)\n",
    "dev = add_edit_distance_features(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd34967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_pos</th>\n",
       "      <th>token_morph</th>\n",
       "      <th>token_dep_label</th>\n",
       "      <th>token_edges</th>\n",
       "      <th>token_wrong</th>\n",
       "      <th>block_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>user</th>\n",
       "      <th>...</th>\n",
       "      <th>format</th>\n",
       "      <th>time</th>\n",
       "      <th>l2</th>\n",
       "      <th>l1</th>\n",
       "      <th>uni_lemma</th>\n",
       "      <th>category</th>\n",
       "      <th>growth_rate</th>\n",
       "      <th>median_aoa</th>\n",
       "      <th>edit_l2</th>\n",
       "      <th>edit_l2_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8XTyQUAl0101</td>\n",
       "      <td>Le</td>\n",
       "      <td>DET</td>\n",
       "      <td>Definite=Def|Gender=Masc|Number=Sing|fPOS=DET++</td>\n",
       "      <td>det</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The boy</td>\n",
       "      <td>YjS/mQOx</td>\n",
       "      <td>...</td>\n",
       "      <td>reverse_translate</td>\n",
       "      <td>14.0</td>\n",
       "      <td>fr</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8XTyQUAl0102</td>\n",
       "      <td>garçon</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Gender=Masc|Number=Sing|fPOS=NOUN++</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The boy</td>\n",
       "      <td>YjS/mQOx</td>\n",
       "      <td>...</td>\n",
       "      <td>reverse_translate</td>\n",
       "      <td>14.0</td>\n",
       "      <td>fr</td>\n",
       "      <td>en</td>\n",
       "      <td>boy</td>\n",
       "      <td>people</td>\n",
       "      <td>0.226911</td>\n",
       "      <td>24.350613</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8XTyQUAl0201</td>\n",
       "      <td>Je</td>\n",
       "      <td>PRON</td>\n",
       "      <td>Number=Sing|Person=1|PronType=Prs|fPOS=PRON++</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>I am a woman.</td>\n",
       "      <td>YjS/mQOx</td>\n",
       "      <td>...</td>\n",
       "      <td>reverse_translate</td>\n",
       "      <td>14.0</td>\n",
       "      <td>fr</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8XTyQUAl0202</td>\n",
       "      <td>suis</td>\n",
       "      <td>VERB</td>\n",
       "      <td>Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbF...</td>\n",
       "      <td>cop</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>I am a woman.</td>\n",
       "      <td>YjS/mQOx</td>\n",
       "      <td>...</td>\n",
       "      <td>reverse_translate</td>\n",
       "      <td>14.0</td>\n",
       "      <td>fr</td>\n",
       "      <td>en</td>\n",
       "      <td>am</td>\n",
       "      <td>helping_verbs</td>\n",
       "      <td>0.294798</td>\n",
       "      <td>29.568209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8XTyQUAl0203</td>\n",
       "      <td>une</td>\n",
       "      <td>DET</td>\n",
       "      <td>Definite=Ind|Gender=Fem|Number=Sing|PronType=D...</td>\n",
       "      <td>det</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>I am a woman.</td>\n",
       "      <td>YjS/mQOx</td>\n",
       "      <td>...</td>\n",
       "      <td>reverse_translate</td>\n",
       "      <td>14.0</td>\n",
       "      <td>fr</td>\n",
       "      <td>en</td>\n",
       "      <td>a</td>\n",
       "      <td>quantifiers</td>\n",
       "      <td>0.239488</td>\n",
       "      <td>27.849093</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       token_id   token token_pos  \\\n",
       "0  8XTyQUAl0101      Le       DET   \n",
       "1  8XTyQUAl0102  garçon      NOUN   \n",
       "2  8XTyQUAl0201      Je      PRON   \n",
       "3  8XTyQUAl0202    suis      VERB   \n",
       "4  8XTyQUAl0203     une       DET   \n",
       "\n",
       "                                         token_morph token_dep_label  \\\n",
       "0    Definite=Def|Gender=Masc|Number=Sing|fPOS=DET++             det   \n",
       "1                Gender=Masc|Number=Sing|fPOS=NOUN++            ROOT   \n",
       "2      Number=Sing|Person=1|PronType=Prs|fPOS=PRON++           nsubj   \n",
       "3  Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbF...             cop   \n",
       "4  Definite=Ind|Gender=Fem|Number=Sing|PronType=D...             det   \n",
       "\n",
       "  token_edges  token_wrong  block_id         prompt      user  ...  \\\n",
       "0           2            0         1        The boy  YjS/mQOx  ...   \n",
       "1           0            0         1        The boy  YjS/mQOx  ...   \n",
       "2           4            0         2  I am a woman.  YjS/mQOx  ...   \n",
       "3           4            0         2  I am a woman.  YjS/mQOx  ...   \n",
       "4           4            0         2  I am a woman.  YjS/mQOx  ...   \n",
       "\n",
       "              format  time  l2  l1 uni_lemma       category growth_rate  \\\n",
       "0  reverse_translate  14.0  fr  en      None           None         NaN   \n",
       "1  reverse_translate  14.0  fr  en       boy         people    0.226911   \n",
       "2  reverse_translate  14.0  fr  en      None           None         NaN   \n",
       "3  reverse_translate  14.0  fr  en        am  helping_verbs    0.294798   \n",
       "4  reverse_translate  14.0  fr  en         a    quantifiers    0.239488   \n",
       "\n",
       "  median_aoa edit_l2 edit_l2_frac  \n",
       "0        NaN     NaN          NaN  \n",
       "1  24.350613     5.0     0.833333  \n",
       "2        NaN     NaN          NaN  \n",
       "3  29.568209     4.0     1.000000  \n",
       "4  27.849093     5.0     0.833333  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9522f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['token_id', 'token', 'token_pos', 'token_morph', 'token_dep_label',\n",
       "       'token_edges', 'token_wrong', 'block_id', 'prompt', 'user',\n",
       "       'countries', 'days', 'client', 'session', 'format', 'time', 'l2',\n",
       "       'l1', 'uni_lemma', 'category', 'growth_rate', 'median_aoa',\n",
       "       'edit_l2', 'edit_l2_frac'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33d57aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_id            object\n",
      "token               object\n",
      "token_pos           object\n",
      "token_morph         object\n",
      "token_dep_label     object\n",
      "token_edges         object\n",
      "token_wrong          int64\n",
      "block_id             int64\n",
      "prompt              object\n",
      "user                object\n",
      "countries           object\n",
      "days               float64\n",
      "client              object\n",
      "session             object\n",
      "format              object\n",
      "time               float64\n",
      "l2                  object\n",
      "l1                  object\n",
      "uni_lemma           object\n",
      "category            object\n",
      "growth_rate        float64\n",
      "median_aoa         float64\n",
      "edit_l2            float64\n",
      "edit_l2_frac       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c72520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select predictors and dependent variable\n",
    "predictors = ['median_aoa', 'edit_l2_frac', 'user', 'days', 'growth_rate']\n",
    "target = 'token_wrong'\n",
    "\n",
    "\n",
    "# Encode categoricals\n",
    "train['user'] = train['user'].astype('category')\n",
    "\n",
    "categorical_features = ['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LightGBM dataset\n",
    "lgb_train = lgb.Dataset(train[predictors], label=train[target], categorical_feature=categorical_features)\n",
    "lgb_val = lgb.Dataset(dev[predictors],     label=dev[target],   categorical_feature=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "617214d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 512,\n",
    "    'min_data_in_leaf': 100,\n",
    "    \"cat_smooth\": 200,\n",
    "    'feature_fraction': 0.7,\n",
    "    'verbose': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109483c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[32m     12\u001b[39m y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
      "\u001b[31mTypeError\u001b[39m: train() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=500,\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(dev[predictors], num_iteration=model.best_iteration)\n",
    "auc = roc_auc_score(dev[target], y_pred)\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
