{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pip install to get libraries:\n",
    "%pip install editdistance unidecode\n",
    "\n",
    "# Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "import editdistance\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "PARQUET_TRAIN = \"data/processed/train_slam_with_features.parquet\"\n",
    "PARQUET_DEV   = \"data/processed/dev_slam_with_features.parquet\"\n",
    "PARQUET_TEST  = \"data/processed/test_slam_with_features.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "train = pd.read_parquet(PARQUET_TRAIN)\n",
    "dev   = pd.read_parquet(PARQUET_DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word Comparisons File\n",
    "trans = pd.read_csv(\"data/processed/language_translation_table.csv\")\n",
    "\n",
    "# maps from translation table\n",
    "en_map = dict(zip(trans[\"uni_lemma\"], trans[\"English\"].astype(str)))\n",
    "fr_map = dict(zip(trans[\"uni_lemma\"], trans[\"French\"].astype(str)))\n",
    "es_map = dict(zip(trans[\"uni_lemma\"], trans[\"Spanish\"].astype(str)))\n",
    "\n",
    "def norm(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = unidecode(str(s)).lower().strip()\n",
    "    parts = s.split()\n",
    "    if len(parts) == 2 and parts[0] in {\"to\",\"i\",\"we\",\"you\",\"they\",\"he\",\"she\",\"the\",\"i'll\"}:\n",
    "        s = parts[1]\n",
    "    elif len(parts) > 2 and len(parts) > 1 and parts[1] == \"will\":\n",
    "        s = \" \".join(parts[2:])\n",
    "    return s\n",
    " \n",
    "def add_edit_distance_features(df):\n",
    "    key = df[\"uni_lemma\"]\n",
    "\n",
    "    # english and target by l2\n",
    "    eng = key.map(en_map)\n",
    "    tgt = np.where(df[\"l2\"].eq(\"fr\"), key.map(fr_map),\n",
    "        np.where(df[\"l2\"].eq(\"es\"), key.map(es_map), np.nan))\n",
    "\n",
    "    eng_n = pd.Series(eng).map(norm)\n",
    "    tgt_n = pd.Series(tgt).map(norm)\n",
    "\n",
    "    mask = eng_n.notna() & tgt_n.notna()\n",
    "    dist = np.full(len(df), np.nan, dtype=float)\n",
    "    dist[mask.values] = [editdistance.eval(a, b) for a, b in zip(eng_n[mask], tgt_n[mask])]\n",
    "    maxlen = np.maximum(eng_n.str.len(), tgt_n.str.len()).replace(0, 1)\n",
    "    frac = dist / maxlen.to_numpy()\n",
    "\n",
    "    df[\"edit_l2\"] = dist\n",
    "    df[\"edit_l2_frac\"] = frac\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_edit_distance_features(train)\n",
    "dev = add_edit_distance_features(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select predictors and dependent variable\n",
    "predictors = ['median_aoa', 'edit_l2_frac', 'user', 'days', 'growth_rate']\n",
    "target = 'token_wrong'\n",
    "\n",
    "\n",
    "# Encode categoricals\n",
    "train['user'] = train['user'].astype('category')\n",
    "dev['user'] = dev['user'].astype('category')\n",
    "\n",
    "categorical_features = ['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LightGBM dataset\n",
    "lgb_train = lgb.Dataset(train[predictors], label=train[target], categorical_feature=categorical_features)\n",
    "lgb_val = lgb.Dataset(dev[predictors],     label=dev[target],   categorical_feature=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 512,\n",
    "    'min_data_in_leaf': 100,\n",
    "    \"cat_smooth\": 200,\n",
    "    'feature_fraction': 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    num_boost_round=500,\n",
    "    callbacks=[\n",
    "        lgb.log_evaluation(period=50),           # print every 50 iters\n",
    "        lgb.early_stopping(stopping_rounds=50),  # set best_iteration\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check parameters settings\n",
    "print(\"Trees trained:\", model.num_trees()) \n",
    "print(\"Current iteration:\", model.current_iteration())\n",
    "print(\"Best iteration:\", model.best_iteration)\n",
    "print(\"Eval results:\", model.best_score)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(dev[predictors], num_iteration=model.best_iteration)\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "auc = metrics.roc_auc_score(dev[target], y_pred)\n",
    "logloss = metrics.log_loss(dev[target], y_pred)\n",
    "accuracy = metrics.accuracy_score(dev[target], y_pred_label)\n",
    "precision = metrics.precision_score(dev[target], y_pred_label)\n",
    "recall = metrics.recall_score(dev[target], y_pred_label)\n",
    "f1 = metrics.f1_score(dev[target], y_pred_label)\n",
    "conf_matrix = metrics.confusion_matrix(dev[target], y_pred_label)\n",
    "\n",
    "print(\"===== Evaluation Metrics =====\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Log Loss:   {logloss:.4f}\")\n",
    "print(f\"Accuracy:   {accuracy:.4f}\")\n",
    "print(f\"Precision:  {precision:.4f}\")\n",
    "print(f\"Recall:     {recall:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
