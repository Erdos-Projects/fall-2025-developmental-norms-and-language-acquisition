{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ecd30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac81bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "LANG = \"en_es\"          # \"en_es\", \"fr_en\", \"es_en\", or \"all\"\n",
    "N_USERS = None          # e.g., 200 for a quick smoke test, or None for all\n",
    "ID_COL = 'token_id'\n",
    "LABEL_COL = 'token_wrong' \n",
    "PARQUET_TRAIN = \"../00_data/clean/train_slam_with_features.parquet\"\n",
    "PARQUET_DEV   = \"../00_data/clean/dev_slam_with_features.parquet\"\n",
    "PARQUET_TEST  = \"../00_data/clean/test_slam_with_features.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  NYU Parameters ---\n",
    "params = {\n",
    "    'fr_en': {\n",
    "        'application': 'binary', 'metric': 'auc', 'learning_rate': .05,\n",
    "        'num_leaves': 256, 'min_data_in_leaf': 100, 'num_boost_round': 750,\n",
    "        'cat_smooth': 200, 'feature_fraction': .7,\n",
    "    },\n",
    "    'en_es': {\n",
    "        'application': 'binary', 'metric': 'auc', 'learning_rate': .05,\n",
    "        'num_leaves': 512, 'min_data_in_leaf': 100, 'num_boost_round': 650,\n",
    "        'cat_smooth': 200, 'feature_fraction': .7,\n",
    "    },\n",
    "    'es_en': {\n",
    "        'application': 'binary', 'metric': 'auc', 'learning_rate': .05,\n",
    "        'num_leaves': 512, 'min_data_in_leaf': 100, 'num_boost_round': 600,\n",
    "        'cat_smooth': 200, 'feature_fraction': .7,\n",
    "    },\n",
    "    'all': {\n",
    "        'application': 'binary', 'metric': 'auc', 'learning_rate': .05,\n",
    "        'num_leaves': 1024, 'min_data_in_leaf': 100, 'num_boost_round': 750,\n",
    "        'cat_smooth': 200, 'max_cat_threshold': 64, 'feature_fraction': .7,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf46fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD THE PARQUETS ---\n",
    "train = pd.read_parquet(PARQUET_TRAIN)\n",
    "dev   = pd.read_parquet(PARQUET_DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6f9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: filter to language pair to mimic NYU 'lang' flag\n",
    "if LANG != \"all\":\n",
    "    l2_val, l1_val = LANG.split('_', 1)  # e.g., 'fr_en' -> fr, en\n",
    "    train = train[(train['l2'] == l2_val) & (train['l1'] == l1_val)]\n",
    "    dev   = dev[(dev['l2']   == l2_val) & (dev['l1']   == l1_val)]\n",
    "\n",
    "# Optional: take only the first N users (to match NYU \"users\" arg for quick tests)\n",
    "if N_USERS is not None:\n",
    "    keep_users = train['user'].drop_duplicates().head(N_USERS)\n",
    "    train = train[train['user'].isin(keep_users)]\n",
    "    dev   = dev[dev['user'].isin(keep_users)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbd4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[LABEL_COL]\n",
    "y_dev   = dev[LABEL_COL]\n",
    "ids_dev = dev[ID_COL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3539899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BUILD FEATURE DICTS TO MATCH NYU STYLE ---\n",
    "def with_lang(x, lang_prefix):\n",
    "    # NYU appends _{lang} to token/root; we’ll use the L2 (target) code\n",
    "    return (str(x).lower() + '_' + lang_prefix) if pd.notnull(x) else '_none_' + '_' + lang_prefix\n",
    "\n",
    "def row_to_feat(row, lang_prefix):\n",
    "    d = {}\n",
    "\n",
    "    # Core categoricals that NYU treats as categorical ids (later int-coded):\n",
    "    # 'token', 'root', 'user'\n",
    "    token_full = with_lang(row.get('token', ''), lang_prefix)\n",
    "    # Use 'uni_lemma' as a stand-in for 'root' (closest analogue in your files)\n",
    "    root_src = row.get('uni_lemma', row.get('token', ''))\n",
    "    root_full = with_lang(root_src, lang_prefix)\n",
    "    d['token'] = token_full\n",
    "    d['root']  = root_full\n",
    "    d['user']  = str(row.get('user', ''))\n",
    "\n",
    "    # Exercise-level “one-hot string” fields:\n",
    "    # NYU uses 'format:xxx', 'session:xxx', 'client:xxx' keys\n",
    "    if pd.notnull(row.get('format')):\n",
    "        d[f'format:{row[\"format\"]}'] = 1.0\n",
    "    if pd.notnull(row.get('session')):\n",
    "        d[f'session:{row[\"session\"]}'] = 1.0\n",
    "    if pd.notnull(row.get('client')):\n",
    "        d[f'client:{row[\"client\"]}'] = 1.0\n",
    "\n",
    "    # Numeric exercise-level features\n",
    "    if pd.notnull(row.get('days')):\n",
    "        d['days'] = float(row['days'])\n",
    "    if pd.notnull(row.get('time')):\n",
    "        try:\n",
    "            d['time'] = float(row['time'])\n",
    "        except Exception:\n",
    "            pass  # keep going if time is null-like\n",
    "\n",
    "    # Token-level: NYU adds part_of_speech, dependency_label one-hots\n",
    "    if pd.notnull(row.get('token_pos')):\n",
    "        d[f'part_of_speech:{row[\"token_pos\"]}'] = 1.0\n",
    "    if pd.notnull(row.get('token_dep_label')):\n",
    "        d[f'dependency_label:{row[\"token_dep_label\"]}'] = 1.0\n",
    "\n",
    "    # Morphology: expand \"key=value|key=value\" → \"morphological_feature:key_value\"\n",
    "    morph = row.get('token_morph')\n",
    "    if pd.notnull(morph):\n",
    "        for kv in str(morph).split('|'):\n",
    "            if '=' in kv:\n",
    "                k, v = kv.split('=', 1)\n",
    "                d[f'morphological_feature:{k}_{v}'] = 1.0\n",
    "\n",
    "    # (If you later add prev/next/parseroot tokens, include as:\n",
    "    # d['prev_token']=..., d['next_token']=..., d['parseroot_token']=... )\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb64fbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['token_id', 'token', 'token_pos', 'token_morph', 'token_dep_label',\n",
       "       'token_edges', 'token_wrong', 'block_id', 'prompt', 'user',\n",
       "       'countries', 'days', 'client', 'session', 'format', 'time', 'l2',\n",
       "       'l1', 'uni_lemma', 'category', 'growth_rate', 'median_aoa'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9946d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_prefix = (LANG[:2] if LANG != 'all' else 'xx')  # NYU uses 2-letter code\n",
    "'''train_dicts = [row_to_feat(r, lang_prefix) for _, r in train.iterrows()]\n",
    "dev_dicts   = [row_to_feat(r, lang_prefix) for _, r in dev.iterrows()]\n",
    "\n",
    "# --- MAP BIG CATEGORICALS TO INTEGER IDS (NYU DOES THIS BEFORE DictVectorizer) ---\n",
    "cat_keys = ['token', 'root', 'user']  # add 'prev_token','next_token','parseroot_token' if you later include them\n",
    "value_maps = {k: {} for k in cat_keys}\n",
    "for k in cat_keys:\n",
    "    next_id = 0\n",
    "    # combine train+dev to share the same id map\n",
    "    for d in (train_dicts + dev_dicts):\n",
    "        if k in d:\n",
    "            v = d[k]\n",
    "            if v not in value_maps[k]:\n",
    "                value_maps[k][v] = next_id\n",
    "                next_id += 1\n",
    "            d[k] = value_maps[k][v]\n",
    "\n",
    "# --- DICTVECTORIZER → SPARSE MATRICES (exactly like NYU) ---\n",
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "X_dev   = dv.transform(dev_dicts)\n",
    "feature_names = dv.feature_names_'''\n",
    "\n",
    "# --- TRAIN LIGHTGBM (mirrors lightgbm_dev.py) ---\n",
    "p = params[LANG if LANG in params else 'all']\n",
    "#d_train = lgb.Dataset(X_train, label=y_train)\n",
    "#d_valid = lgb.Dataset(X_dev,   label=y_dev)\n",
    "\n",
    "'''feature_cols = [\n",
    "    # include categorical and numeric columns you actually want\n",
    "    'token', 'root', 'user', 'prev_token', 'next_token', 'parseroot_token',\n",
    "]'''\n",
    "\n",
    "feature_cols = [\n",
    "    'token', 'block_id', 'user'\n",
    "]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "X_dev   = dev[feature_cols]\n",
    "\n",
    "\n",
    "# --- Encode categoricals automatically with LightGBM ---\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Make sure we’re not writing into a view\n",
    "X_train = X_train.copy()\n",
    "X_dev   = X_dev.copy()\n",
    "\n",
    "# Convert ALL object cols in train/dev to aligned int codes (joint categories)\n",
    "obj_cols = (\n",
    "    X_train.select_dtypes(include=['object']).columns\n",
    "    .union(X_dev.select_dtypes(include=['object']).columns)\n",
    ")\n",
    "\n",
    "for c in obj_cols:\n",
    "    X_train[c] = X_train[c].astype('category')\n",
    "    X_dev[c]   = X_dev[c].astype('category')\n",
    "    cats = X_train[c].cat.categories.union(X_dev[c].cat.categories)\n",
    "    X_train[c] = X_train[c].cat.set_categories(cats).cat.codes.astype('int32')\n",
    "    X_dev[c]   = X_dev[c].cat.set_categories(cats).cat.codes.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c4f5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_cols)\n",
    "d_valid = lgb.Dataset(X_dev,   label=y_dev,   categorical_feature=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "245fe4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 330788, number of negative: 2292169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3708\n",
      "[LightGBM] [Info] Number of data points in the train set: 2622957, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.126113 -> initscore=-1.935776\n",
      "[LightGBM] [Info] Start training from score -1.935776\n",
      "[25]\ttrain's auc: 0.745084\tvalid's auc: 0.71695\n",
      "[50]\ttrain's auc: 0.75404\tvalid's auc: 0.726334\n",
      "[75]\ttrain's auc: 0.755564\tvalid's auc: 0.727364\n",
      "[100]\ttrain's auc: 0.763055\tvalid's auc: 0.736583\n",
      "[125]\ttrain's auc: 0.764746\tvalid's auc: 0.738608\n",
      "[150]\ttrain's auc: 0.766615\tvalid's auc: 0.740992\n",
      "[175]\ttrain's auc: 0.76721\tvalid's auc: 0.741571\n",
      "[200]\ttrain's auc: 0.768135\tvalid's auc: 0.742442\n",
      "[225]\ttrain's auc: 0.768746\tvalid's auc: 0.742825\n",
      "[250]\ttrain's auc: 0.769369\tvalid's auc: 0.743038\n",
      "[275]\ttrain's auc: 0.770025\tvalid's auc: 0.743068\n",
      "[300]\ttrain's auc: 0.770335\tvalid's auc: 0.743039\n",
      "[325]\ttrain's auc: 0.770895\tvalid's auc: 0.742985\n",
      "[350]\ttrain's auc: 0.771259\tvalid's auc: 0.742948\n",
      "[375]\ttrain's auc: 0.77177\tvalid's auc: 0.742883\n",
      "[400]\ttrain's auc: 0.772326\tvalid's auc: 0.742798\n",
      "[425]\ttrain's auc: 0.772595\tvalid's auc: 0.742726\n",
      "[450]\ttrain's auc: 0.773002\tvalid's auc: 0.74264\n",
      "[475]\ttrain's auc: 0.773413\tvalid's auc: 0.742567\n",
      "[500]\ttrain's auc: 0.77374\tvalid's auc: 0.74248\n",
      "[525]\ttrain's auc: 0.774099\tvalid's auc: 0.742364\n",
      "[550]\ttrain's auc: 0.774372\tvalid's auc: 0.742291\n",
      "[575]\ttrain's auc: 0.774647\tvalid's auc: 0.742223\n",
      "[600]\ttrain's auc: 0.774974\tvalid's auc: 0.742134\n",
      "[625]\ttrain's auc: 0.775368\tvalid's auc: 0.742034\n",
      "[650]\ttrain's auc: 0.775489\tvalid's auc: 0.741998\n",
      "DEV AUC (en_es): 0.741998\n"
     ]
    }
   ],
   "source": [
    "'''bst = lgb.train(\n",
    "    p, d_train, valid_sets=[d_train, d_valid],\n",
    "    valid_names=['train','valid'],\n",
    "    categorical_feature=[k for k in cat_keys if any(fn==k for fn in feature_names)],\n",
    "    num_boost_round=p['num_boost_round'],\n",
    "    verbose_eval=25\n",
    ")'''\n",
    "\n",
    "bst = lgb.train(\n",
    "    p,\n",
    "    d_train,\n",
    "    valid_sets=[d_train, d_valid],\n",
    "    valid_names=['train','valid'],\n",
    "    num_boost_round=p['num_boost_round'],\n",
    "    callbacks=[lgb.log_evaluation(25)]\n",
    ")\n",
    "\n",
    "# --- EVALUATE (AUC on dev) ---\n",
    "dev_pred = bst.predict(X_dev)\n",
    "auc = roc_auc_score(y_dev, dev_pred)\n",
    "print(f\"DEV AUC ({LANG}): {auc:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
